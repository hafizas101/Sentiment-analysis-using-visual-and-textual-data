{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2, time, json, csv, re, pysrt\n",
    "from yolo_files.utils import *\n",
    "from keras.models import load_model\n",
    "from utils.datasets import get_labels\n",
    "from utils.preprocessor import preprocess_input\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "num is a very important variable. It controls how much frames per second of the video we are extracting so\n",
    "basically it is very computationally expensive and time consuming for long videos to perform frame by frame\n",
    "detection and processing. Hence we can periodically skip some frames. This is controlled by num variable. Setting\n",
    "num=1 would mean extract all frames. num = 2 means skip 1 frame and then extract the second and so on. num = 4\n",
    "would mean skip 3 frames and then extract 1 and then again skip 3 and so on.\n",
    "\"\"\"\n",
    "\n",
    "num = 1\n",
    "offset = 30\n",
    "emotion_code_mapping = {0:'angry',1:'disgust',2:'fear',3:'happy', 4:'sad',5:'surprise',6:'neutral'}\n",
    "input_video_path = os.path.join(os.getcwd(), \"input_video.mp4\")\n",
    "output_video_path = os.path.join(os.getcwd(), \"output.mp4\")\n",
    "current_path = os.getcwd()\n",
    "output_FPS = 4\n",
    "\n",
    "# stopword = stopwords.words('english')\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\"\"\"\n",
    "Boundary for slighly and highly negative. Increasing it will increase number of slightly negatives\n",
    "but will decrease number of highly negatives. Same rule for positives.\n",
    "\"\"\"\n",
    "sentiment_threshold = 0.3\n",
    "num_period = 300\n",
    "current_path = os.getcwd()\n",
    "input_subtitle_path = os.path.join(os.getcwd(), \"climate.srt\")\n",
    "emotions = ['Highly Negative', 'Slightly Negative', 'Neutral', 'Slightly Positive', 'Highly Positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_predict(frame, conf, emotion, left, top, right, bottom):\n",
    "    # Draw a bounding box.\n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), COLOR_YELLOW, 2)\n",
    "\n",
    "    text = emotion+' {:.2f}'.format(conf)\n",
    "\n",
    "    # Display the label at the top of the bounding box\n",
    "    label_size, base_line = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "\n",
    "    top = max(top, label_size[1])\n",
    "    cv2.putText(frame, text, (left, top - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_WHITE, 1)\n",
    "    return\n",
    "\n",
    "def convert_frames_to_video(frames):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Be sure to use lower case\n",
    "    height, width, channels = frames[0].shape\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, output_FPS, (width, height))\n",
    "    for i, ff in enumerate (frames):\n",
    "        out.write(ff)\n",
    "    out.release\n",
    "    cv2.destroyAllWindows()\n",
    "    return\n",
    "\n",
    "def clean_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "#     removing_stopwords = [word for word in word_tokens if word not in stopword]\n",
    "#     lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in removing_stopwords]\n",
    "    wordsa=[word.lower() for word in words if word.isalpha()]\n",
    "    return wordsa\n",
    "\n",
    "\n",
    "def calculate_polarity_and_intensity(d, rep):\n",
    "    if d['neu']==1.0:\n",
    "        emotional_intensity = \"Neutral\"\n",
    "        polarity = 0.0\n",
    "        rep[2] = rep[2] + 1\n",
    "    elif ((d['neg'] < sentiment_threshold) & (d['pos'] == 0.0)):\n",
    "        emotional_intensity = \"Slightly Negative\"\n",
    "        polarity = -1*d['neg']\n",
    "        rep[1] = rep[1] + 1\n",
    "    elif ((d['neg'] > sentiment_threshold) & (d['pos'] == 0.0)):\n",
    "        emotional_intensity = \"Highly Negative\"\n",
    "        polarity = -1*d['neg']\n",
    "        rep[0] = rep[0] + 1\n",
    "    elif ((d['neg'] == 0.0) & (d['pos'] < sentiment_threshold)):\n",
    "        emotional_intensity = \"Slightly Positive\"\n",
    "        polarity = d['pos']\n",
    "        rep[3] = rep[3] + 1\n",
    "    elif ((d['neg'] == 0.0) & (d['pos'] > sentiment_threshold)):\n",
    "        emotional_intensity = \"Highly Positive\"\n",
    "        polarity = d['pos']\n",
    "        rep[4] = rep[4] + 1\n",
    "    else:\n",
    "        values = [d['neg'], d['neu'], d['pos']]\n",
    "        dominant = max(values)\n",
    "        \n",
    "        if values.index(dominant) == 0:\n",
    "            polarity = -1*d['neg']\n",
    "            if dominant > sentiment_threshold:\n",
    "                emotional_intensity = \"Highly Negative\"\n",
    "                rep[0] = rep[0] + 1\n",
    "            else:\n",
    "                emotional_intensity = \"Slightly Negative\"\n",
    "                rep[1] = rep[1] + 1\n",
    "        elif values.index(dominant) == 1:\n",
    "            polarity = 0.0\n",
    "            emotional_intensity = \"Neutral\"\n",
    "            rep[0] = rep[0] + 1\n",
    "        else:\n",
    "            polarity = d['pos']\n",
    "            if dominant < sentiment_threshold:\n",
    "                emotional_intensity = \"Slightly Positive\"\n",
    "                rep[3] = rep[3] + 1\n",
    "            else:\n",
    "                emotional_intensity = \"Highly Positive\"\n",
    "                rep[4] = rep[4] + 1\n",
    "    return polarity, emotional_intensity, rep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_time_stamp(l):\n",
    "    if l[:2].isnumeric() and l[2] == ':':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_letters(line):\n",
    "    if re.search('[a-zA-Z]', line):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_no_text(line):\n",
    "    l = line.strip()\n",
    "    if not len(l):\n",
    "        return True\n",
    "    if l.isnumeric():\n",
    "        return True\n",
    "    if is_time_stamp(l):\n",
    "        return True\n",
    "    if l[0] == '(' and l[-1] == ')':\n",
    "        return True\n",
    "    if not has_letters(line):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_lowercase_letter_or_comma(letter):\n",
    "    if letter.isalpha() and letter.lower() == letter:\n",
    "        return True\n",
    "    if letter == ',':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_up(lines):\n",
    "    \"\"\"\n",
    "    Get rid of all non-text lines and\n",
    "    try to combine text broken into multiple lines\n",
    "    \"\"\"\n",
    "    new_lines = []\n",
    "    for line in lines[1:]:\n",
    "        if has_no_text(line):\n",
    "            continue\n",
    "        elif len(new_lines) and is_lowercase_letter_or_comma(line[0]):\n",
    "            new_lines[-1] = new_lines[-1].strip() + ' ' + line\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod-et-emo-001 ==> Video Splitting in Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Per Second: 25.0\n",
      "Total number of frames read: 153\n",
      "Time taken in reading the frames: 0.38521599769592285 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "FPS = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(\"Frames Per Second: \"+str(FPS))\n",
    "\n",
    "frames = []\n",
    "start = time.time()\n",
    "count = 0\n",
    "frame_originals = []\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    count = count + 1\n",
    "    if not ret:\n",
    "        break\n",
    "    if count % num == 0:        \n",
    "        frames.append(frame)\n",
    "#         cv2.imshow(\"Original\", frame)\n",
    "#         cv2.waitKey(0)\n",
    "end = time.time()\n",
    "print(\"Total number of frames read: \"+str(len(frames)))\n",
    "print(\"Time taken in reading the frames: {} seconds\".format(end-start))\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod-et-emo-002 ==> Faces and mood detection per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arslan/.conda/envs/mood3/lib/python3.7/site-packages/keras/engine/saving.py:350: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in processing 153 frames: 120.295 seconds\n"
     ]
    }
   ],
   "source": [
    "cfg = './yolo_files/yolov3-face.cfg'\n",
    "weights = './yolo_files/yolov3-wider_16000.weights'\n",
    "\n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(cfg, weights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "faces = []\n",
    "\n",
    "\n",
    "emotion_model_path = './models/emotion_model.hdf5'\n",
    "# emotion_labels = get_labels('fer2013')\n",
    "# print(emotion_labels)\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "count_repetition = np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "X_position = []\n",
    "Y_position = []\n",
    "Frame = []\n",
    "Width = []\n",
    "Height = []\n",
    "Emotion_code = []\n",
    "Emotion_name = []\n",
    "Score = []\n",
    "start = time.time()\n",
    "for i in range (len(frames)):\n",
    "    blob = cv2.dnn.blobFromImage(frames[i], 1 / 255, (IMG_WIDTH, IMG_HEIGHT),[0, 0, 0], 1, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(get_outputs_names(net))\n",
    "\n",
    "    # Remove the bounding boxes with low confidence\n",
    "    frame_faces = post_process(frames[i], outs, CONF_THRESHOLD, NMS_THRESHOLD)\n",
    "    if len(frame_faces) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        face_imgs = []\n",
    "        for j, f in enumerate(frame_faces):\n",
    "            Frame.append(i+1)\n",
    "            faces.append(f)\n",
    "            [left, top, w, h] = f\n",
    "            X_position.append(left)\n",
    "            Y_position.append(top)\n",
    "            Width.append(w)\n",
    "            Height.append(h)\n",
    "#             original_face = frames[i][top : top+h, left:left+w]\n",
    "#             cv2.imwrite(str(j+1)+\".jpg\", original_face)\n",
    "#             offset_face = frames[i][top-offset : top+h+offset , left-offset:left+w+offset]\n",
    "#             cv2.imwrite(str(j+1)+\"_offset.jpg\", offset_face)\n",
    "            face_imgs.append(frames[i][top-offset : top+h+offset , left-offset:left+w+offset])\n",
    "#             cv2.imshow(\"Faces\", face_imgs[j])\n",
    "#             cv2.waitKey(0)\n",
    "            gray_face = cv2.cvtColor(face_imgs[j], cv2.COLOR_BGR2GRAY)\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "            gray_face = preprocess_input(gray_face, True)\n",
    "            gray_face = np.expand_dims(gray_face, 0)\n",
    "            gray_face = np.expand_dims(gray_face, -1)\n",
    "            emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "            emotion_probability = np.max(emotion_prediction)\n",
    "            Score.append(emotion_probability)\n",
    "            emotion_label_arg = np.argmax(emotion_prediction)\n",
    "            Emotion_code.append(emotion_label_arg)\n",
    "            count_repetition[emotion_label_arg] = count_repetition[emotion_label_arg] + 1\n",
    "            emotion_text = emotion_code_mapping[emotion_label_arg]\n",
    "            Emotion_name.append(emotion_text)\n",
    "            draw_predict(frames[i], emotion_probability, emotion_text, left, top, left+w, top+h)\n",
    "\n",
    "#         cv2.imshow(\"Faces\", frames[i])\n",
    "#         cv2.waitKey(0)\n",
    "end = time.time()\n",
    "print(\"Time taken in processing {0:1.0f} frames: {1:2.3f} seconds\".format(len(frames), end-start))\n",
    "convert_frames_to_video(frames)\n",
    "cv2.destroyAllWindows()\n",
    "project2_dict = {'Frame': Frame, 'X_position': X_position, 'Y_position': Y_position, 'Width': Width, 'Height': Height,\n",
    "                 'Emotion_code': Emotion_code, 'Emotion_name': Emotion_name, 'Score': Score}\n",
    "\n",
    "if os.path.exists(os.path.join(current_path, \"mod-et-emo-002.csv\")):\n",
    "    os.remove(os.path.join(current_path, \"mod-et-emo-002.csv\"))\n",
    "df = pd.DataFrame(data=project2_dict)\n",
    "df.to_csv('mod-et-emo-002.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod-et-emo-003 ==> Aggregation of emotions detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Emotion Code: 4\n",
      "Most Frequent Emotion: sad\n",
      "Final result: 0.5324261444082552\n"
     ]
    }
   ],
   "source": [
    "# print(count_repetition)\n",
    "\n",
    "aggregated_emotion_code = np.argmax(count_repetition)\n",
    "print(\"Aggregated Emotion Code: \"+str(aggregated_emotion_code))\n",
    "aggregated_emotion_name = emotion_code_mapping[aggregated_emotion_code]\n",
    "print(\"Most Frequent Emotion: \"+str(aggregated_emotion_name))\n",
    "num = len(Emotion_code)\n",
    "total_rep = 0\n",
    "total = 0\n",
    "for i in range (num):\n",
    "    total = total + Score[i]*count_repetition[Emotion_code[i]]\n",
    "    total_rep = total_rep + count_repetition[Emotion_code[i]]\n",
    "    \n",
    "final_code = total/total_rep\n",
    "print(\"Final result: \"+str(final_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod-et-emo-004 ==> Mood polarity detection in subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let’s talk about climate change.\n",
      "\n",
      "People are calling it the crisis of our time, and  it is.\n",
      "\n",
      "“climate change\n",
      "\n",
      "“climate change”\n",
      "\n",
      "“climate changes”\n",
      "\n",
      "“climate change”\n",
      "\n",
      "But it’s easy to get lost in this story.\n",
      "\n",
      "The science is dense, and politics get in the way.\n",
      "\n",
      "World leaders are meeting in Madrid to talk about the climate crisis and how to slow it down.\n",
      "\n",
      "And they’re under pressure from millions of people around the world calling for concrete action.\n",
      "\n",
      "The empty promises are the same and the inaction is the same.\n",
      "\n",
      "So what exactly are we doing wrong and how do we fix it?\n",
      "\n",
      "We’re going to kick this off with some basic science.\n",
      "\n",
      "So bear with me, because this is important.\n",
      "\n",
      "Look at this graph.\n",
      "\n",
      "These are the levels of carbon dioxide in our atmosphere over hundreds of thousands of years.\n",
      "\n",
      "But this spike in carbon dioxide at the very end?\n",
      "\n",
      "That took off during the industrial revolution.\n",
      "\n",
      "We started breaking CO2 records in 1950, and we haven’t stopped since. why?\n",
      "\n",
      "Well, scientists say there’s a 95% chance that human activity is the cause.\n",
      "\n",
      "We’ve been burning more and more fossil fuels like oil and coal, which release CO2, to power our homes, factories, airplanes and cars.\n",
      "\n",
      "There’s also a lot more of us. The global population has tripled in the past 70 years.\n",
      "\n",
      "And we’re consuming more products from animals that release another pollutant called Methane.\n",
      "\n",
      "So all those gases are in the air, and when sunlight gets into the earth’s atmosphere, some of the heat gets trapped, and the planet gets warmer.\n",
      "\n",
      "That’s why they call it the “Greenhouse Effect”.\n",
      "\n",
      "But the concern is not that the earth is getting warmer.\n",
      "\n",
      "But the concern is not that the earth is getting warmer.\n",
      "\n",
      "“It’s actually the warmest temperature on Earth since the last ice age since 10 thousand years ago.”\n",
      "\n",
      "The UN says that right now, our world is about 1 degree hotter than pre-industrial times. That’s around the year 1800.\n",
      "\n",
      "Which is okay. In fact, the UN says if we warm by 1.5 degrees before the end of the century we should be fine.\n",
      "\n",
      "The UN says even 2 degrees would 'probably' be alright.\n",
      "\n",
      "But again, the problem is speed. Because right now, we are on track to hit 1.5 degrees in only ten years.\n",
      "\n",
      "And If we don't slow that warming down, it could mean catastrophe within my lifetime, and maybe yours too.\n",
      "\n",
      "And we’re already getting a taste.\n",
      "\n",
      "“Climate change is here. Climate change is happening. We are well into the 6th mass extinction event.”\n",
      "\n",
      "”Europe is currently colder than the Artic.”\n",
      "\n",
      "“More than a thousand people being rescued just in the early morning hours of Sunday.”\n",
      "\n",
      "“Millions of people are likely to suffer worsening food and water shortages.”\n",
      "\n",
      "‘The drought that’s now in its tenth year is a phenomena that’s here to stay.’\n",
      "\n",
      "“We’ve never seen a year’s worth of rain in less than seven days.”\n",
      "\n",
      "Sea levels are rising about 3 millimetres a year because seawater expands as temperatures get warmer.\n",
      "\n",
      "Melting ice sheets and glaciers also add trillions of tons of freshwater into our oceans.\n",
      "\n",
      "People around the world are already losing their homes.\n",
      "\n",
      "And if things carry on, millions more of us will have to pack up too.\n",
      "\n",
      "Entire coastal cities could be underwater within 80 years.\n",
      "\n",
      "Like Miami in the US or Osaka in Japan.\n",
      "\n",
      "Entire island nations in the pacific could completely disappear.\n",
      "\n",
      "“Natural disasters becoming more and more intense, more frequent with devastating consequences. The dramatic impacts of droughts in different parts of the world, all of this is creating a situation that is a real threat to humankind. And we are not doing enough.”\n",
      "\n",
      "“If 99% of doctors said to you, ‘take this medicine, or you will get really sick and probably die’ you would take it , who wouldn’t take it? the problem is, at the moment, we don’t have any medicine.”\n",
      "\n",
      "Now, there is a plan to slow all this down.\n",
      "\n",
      "Back in 2016, world leaders signed the so-called ‘Paris Agreement’.\n",
      "\n",
      "And the big pledge is to cap temperatures rising by 1.5 degrees or a maximum of 2,  before the year 2100.\n",
      "\n",
      "So countries set their own targets on how much CO2 they emit.\n",
      "\n",
      "But here’s the thing\n",
      "\n",
      "Three years after the agreement, global CO2 levels are still going up.\n",
      "\n",
      "“CO2 emissions have been going up the last year by two per cent so that's actually above the average of the last ten years.\n",
      "\n",
      "So it's started to increase again and it doesn't look too good.\n",
      "\n",
      "In some ways, we’re going backwards.\n",
      "\n",
      "\"The United States will cease all implementation of the nonbinding\n",
      "\n",
      "Paris accord and the draconian financial and economic burdens the agreement imposes on our country.\"\n",
      "\n",
      "The US, one of the world’s biggest polluters, has pulled out of the Paris deal. Russia and China are accused of not giving themselves ambitious targets in the first place. ambitious targets in the first place.\n",
      "\n",
      "Turkey and Poland want to build more power plants that use coal.\n",
      "\n",
      "And then there’s the sceptics.\n",
      "\n",
      "It’s a political decision,  that it’s man-made global warming.\n",
      "\n",
      "We forced the computer models to say AHA!\n",
      "\n",
      "Human influence, CO2 and other stuff.\n",
      "\n",
      "“The ground base temperature data has been massaged to show an increase but the satellite data shows no increase.”\n",
      "\n",
      "On the other hand, there is positive momentum. There’s more awareness and some countries are making progress. India, Morocco, and\n",
      "\n",
      "The Gambia have massive renewable energy projects.\n",
      "\n",
      "There are different countries doing different things really successfully some countries are, for example, making all public transport free in the cities. What a great way to encourage people out of their cars.\n",
      "\n",
      "But experts say what’s needed now is an even bigger push to change everything about the way we run our world.\n",
      "\n",
      "Business as usual has got to change. Politics as usual has got to change\n",
      "\n",
      "In order to combat that we have to change the system that has allowed it to happen. You can’t have infinite growth on a finite planet.\n",
      "\n",
      "And everyone can do that by shifting to renewable energy, reducing the use of cars, use trains more, cycle more, eat less meat consume a bit more carefully.\n",
      "\n",
      "So where does that leave us? Well there’s only so much bike-riding and light-bulb replacing you and I can do everyday.\n",
      "\n",
      "But the truth is that it’s those everyday things that are going to change anyway.\n",
      "\n",
      "Even coffee could run out if farmers can’t grow it.\n",
      "\n",
      "So the expert advice? Is that it’s down to all of us, to change our ways and shake things up, or climate change is going to do it for us.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(input_subtitle_path, errors='replace') as f:\n",
    "    lines = f.readlines()\n",
    "    new_lines = clean_up(lines)\n",
    "for i, l in enumerate (new_lines):\n",
    "    print(l)\n",
    "\n",
    "emotional_intensity = []\n",
    "polarity = []\n",
    "sentences = []\n",
    "count_repitition = [0, 0, 0, 0, 0]\n",
    "count = 0\n",
    "for i, line in enumerate(new_lines):\n",
    "    words = clean_words(line)\n",
    "    data = \"\"\n",
    "    for j, w in enumerate(words):\n",
    "        data = data + \" \"+ w    \n",
    "    \n",
    "    if data == \"\":\n",
    "        continue\n",
    "    sents = nltk.sent_tokenize(data)\n",
    "    for s in sents:\n",
    "        sentences.append(s)\n",
    "        d = analyzer.polarity_scores(s)\n",
    "        p, e, count_repitition = calculate_polarity_and_intensity(d, count_repitition)\n",
    "        polarity.append(p)\n",
    "        emotional_intensity.append(e)\n",
    "\n",
    "if os.path.exists(os.path.join(current_path, \"mod-et-emo-004.csv\")):\n",
    "    os.remove(os.path.join(current_path, \"mod-et-emo-004.csv\"))\n",
    "if len(sentences) == len(emotional_intensity):\n",
    "    df = pd.DataFrame(data = {'Sentences': sentences, 'Emotional Intensity': emotional_intensity, 'Polarity': polarity})\n",
    "    df.to_csv('mod-et-emo-004.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mod-et-emo-005 ==> Association of the temporary emotional intensity in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1103\n",
      "Time period in milliseconds: 1356\n",
      "Words per time period: 4\n",
      " let s talk about\n",
      " climate change people are\n",
      " calling it the crisis\n",
      " of our time and\n",
      " it is climate change\n",
      " climate change climate changes\n",
      " climate change but it\n",
      " s easy to get\n",
      " lost in this story\n",
      " the science is dense\n",
      " and politics get in\n",
      " the way world leaders\n",
      " are meeting in madrid\n",
      " to talk about the\n",
      " climate crisis and how\n",
      " to slow it down\n",
      " and they re under\n",
      " pressure from millions of\n",
      " people around the world\n",
      " calling for concrete action\n",
      " the empty promises are\n",
      " the same and the\n",
      " inaction is the same\n",
      " so what exactly are\n",
      " we doing wrong and\n",
      " how do we fix\n",
      " it we re going\n",
      " to kick this off\n",
      " with some basic science\n",
      " so bear with me\n",
      " because this is important\n",
      " look at this graph\n",
      " these are the levels\n",
      " of carbon dioxide in\n",
      " our atmosphere over hundreds\n",
      " of thousands of years\n",
      " but this spike in\n",
      " carbon dioxide at the\n",
      " very end that took\n",
      " off during the industrial\n",
      " revolution we started breaking\n",
      " records in and we\n",
      " haven t stopped since\n",
      " why well scientists say\n",
      " there s a chance\n",
      " that human activity is\n",
      " the cause we ve\n",
      " been burning more and\n",
      " more fossil fuels like\n",
      " oil and coal which\n",
      " release to power our\n",
      " homes factories airplanes and\n",
      " cars there s also\n",
      " a lot more of\n",
      " us the global population\n",
      " has tripled in the\n",
      " past years and we\n",
      " re consuming more products\n",
      " from animals that release\n",
      " another pollutant called methane\n",
      " so all those gases\n",
      " are in the air\n",
      " and when sunlight gets\n",
      " into the earth s\n",
      " atmosphere some of the\n",
      " heat gets trapped and\n",
      " the planet gets warmer\n",
      " that s why they\n",
      " call it the greenhouse\n",
      " effect but the concern\n",
      " is not that the\n",
      " earth is getting warmer\n",
      " but the concern is\n",
      " not that the earth\n",
      " is getting warmer it\n",
      " s actually the warmest\n",
      " temperature on earth since\n",
      " the last ice age\n",
      " since thousand years the\n",
      " un says that right\n",
      " now our world is\n",
      " about degree hotter than\n",
      " times that s around\n",
      " the year which is\n",
      " okay in fact the\n",
      " un says if we\n",
      " warm by degrees before\n",
      " the end of the\n",
      " century we should be\n",
      " fine the un says\n",
      " even degrees would be\n",
      " alright but again the\n",
      " problem is speed because\n",
      " right now we are\n",
      " on track to hit\n",
      " degrees in only ten\n",
      " years and if we\n",
      " do slow that warming\n",
      " down it could mean\n",
      " catastrophe within my lifetime\n",
      " and maybe yours too\n",
      " and we re already\n",
      " getting a taste climate\n",
      " change is here climate\n",
      " change is happening we\n",
      " are well into the\n",
      " mass extinction europe is\n",
      " currently colder than the\n",
      " more than a thousand\n",
      " people being rescued just\n",
      " in the early morning\n",
      " hours of millions of\n",
      " people are likely to\n",
      " suffer worsening food and\n",
      " water the drought that\n",
      " s now in its\n",
      " tenth year is a\n",
      " phenomena that s here\n",
      " to we ve never\n",
      " seen a year s\n",
      " worth of rain in\n",
      " less than seven sea\n",
      " levels are rising about\n",
      " millimetres a year because\n",
      " seawater expands as temperatures\n",
      " get warmer melting ice\n",
      " sheets and glaciers also\n",
      " add trillions of tons\n",
      " of freshwater into our\n",
      " oceans people around the\n",
      " world are already losing\n",
      " their homes and if\n",
      " things carry on millions\n",
      " more of us will\n",
      " have to pack up\n",
      " too entire coastal cities\n",
      " could be underwater within\n",
      " years like miami in\n",
      " the us or osaka\n",
      " in japan entire island\n",
      " nations in the pacific\n",
      " could completely disappear natural\n",
      " disasters becoming more and\n",
      " more intense more frequent\n",
      " with devastating consequences the\n",
      " dramatic impacts of droughts\n",
      " in different parts of\n",
      " the world all of\n",
      " this is creating a\n",
      " situation that is a\n",
      " real threat to humankind\n",
      " and we are not\n",
      " doing if of doctors\n",
      " said to you take\n",
      " this medicine or you\n",
      " will get really sick\n",
      " and probably die you\n",
      " would take it who\n",
      " wouldn t take it\n",
      " the problem is at\n",
      " the moment we don\n",
      " t have any now\n",
      " there is a plan\n",
      " to slow all this\n",
      " down back in world\n",
      " leaders signed the paris\n",
      " agreement and the big\n",
      " pledge is to cap\n",
      " temperatures rising by degrees\n",
      " or a maximum of\n",
      " before the year so\n",
      " countries set their own\n",
      " targets on how much\n",
      " they emit but here\n",
      " s the thing three\n",
      " years after the agreement\n",
      " global levels are still\n",
      " going up emissions have\n",
      " been going up the\n",
      " last year by two\n",
      " per cent so that\n",
      " actually above the average\n",
      " of the last ten\n",
      " years so it started\n",
      " to increase again and\n",
      " it does look too\n",
      " good in some ways\n",
      " we re going backwards\n",
      " the united states will\n",
      " cease all implementation of\n",
      " the nonbinding paris accord\n",
      " and the draconian financial\n",
      " and economic burdens the\n",
      " agreement imposes on our\n",
      " country the us one\n",
      " of the world s\n",
      " biggest polluters has pulled\n",
      " out of the paris\n",
      " deal russia and china\n",
      " are accused of not\n",
      " giving themselves ambitious targets\n",
      " in the first place\n",
      " ambitious targets in the\n",
      " first place turkey and\n",
      " poland want to build\n",
      " more power plants that\n",
      " use coal and then\n",
      " there s the sceptics\n",
      " it s a political\n",
      " decision that it s\n",
      " global warming we forced\n",
      " the computer models to\n",
      " say aha human influence\n",
      " and other stuff the\n",
      " ground base temperature data\n",
      " has been massaged to\n",
      " show an increase but\n",
      " the satellite data shows\n",
      " no on the other\n",
      " hand there is positive\n",
      " momentum there s more\n",
      " awareness and some countries\n",
      " are making progress india\n",
      " morocco and the gambia\n",
      " have massive renewable energy\n",
      " projects there are different\n",
      " countries doing different things\n",
      " really successfully some countries\n",
      " are for example making\n",
      " all public transport free\n",
      " in the cities what\n",
      " a great way to\n",
      " encourage people out of\n",
      " their cars but experts\n",
      " say what s needed\n",
      " now is an even\n",
      " bigger push to change\n",
      " everything about the way\n",
      " we run our world\n",
      " business as usual has\n",
      " got to change politics\n",
      " as usual has got\n",
      " to change in order\n",
      " to combat that we\n",
      " have to change the\n",
      " system that has allowed\n",
      " it to happen you\n",
      " can t have infinite\n",
      " growth on a finite\n",
      " planet and everyone can\n",
      " do that by shifting\n",
      " to renewable energy reducing\n",
      " the use of cars\n",
      " use trains more cycle\n",
      " more eat less meat\n",
      " consume a bit more\n",
      " carefully so where does\n",
      " that leave us well\n",
      " there s only so\n",
      " much and replacing you\n",
      " and i can do\n",
      " everyday but the truth\n",
      " is that it s\n",
      " those everyday things that\n",
      " are going to change\n",
      " anyway even coffee could\n",
      " run out if farmers\n",
      " can t grow it\n",
      " so the expert advice\n",
      " is that it s\n",
      " down to all of\n",
      " us to change our\n",
      " ways and shake things\n",
      " up or climate change\n",
      " is going to do\n",
      " it for us\n"
     ]
    }
   ],
   "source": [
    "subs = pysrt.open(input_subtitle_path)\n",
    "x = subs[len(subs)-1]\n",
    "[hour, minute, sec] = [x.end.hours, x.end.minutes, x.end.seconds]\n",
    "total_duration = hour*3600000 + minute*60000 + sec*1000\n",
    "\n",
    "text = \"\"\n",
    "for i in range (len(subs)):\n",
    "    a = subs[i]\n",
    "    text = text +\" \"+ a.text\n",
    "    \n",
    "words = clean_words(text)\n",
    "num_words = len(words)\n",
    "print(\"Total words: \"+str(num_words))\n",
    "words_per_duration = num_words/total_duration\n",
    "time_period = int(total_duration/num_period)\n",
    "print(\"Time period in milliseconds: \"+str(time_period))\n",
    "words_period = int(time_period*words_per_duration)\n",
    "print(\"Words per time period: \"+str(words_period+1))\n",
    "\n",
    "period_num = []\n",
    "from_to = []\n",
    "polarity = []\n",
    "emotional_intensity = []\n",
    "strings = []\n",
    "starting = 0\n",
    "ending = time_period\n",
    "\n",
    "start = 0\n",
    "stop = words_period+1\n",
    "count_repitition = [0, 0, 0, 0, 0]\n",
    "for i in range(num_period):\n",
    "    all_words = words[start:stop]\n",
    "    data = \"\"\n",
    "    for j, w in enumerate(all_words):\n",
    "        data = data + \" \"+ w    \n",
    "    \n",
    "    if data == \"\":\n",
    "        continue\n",
    "    period_num.append(i)\n",
    "    from_to.append((starting, ending))\n",
    "    strings.append(data)\n",
    "    print(data)\n",
    "    d = analyzer.polarity_scores(data)\n",
    "    p, e, count_repitition = calculate_polarity_and_intensity(d, count_repitition)\n",
    "    polarity.append(p)\n",
    "    emotional_intensity.append(e)    \n",
    "    \n",
    "    start = stop\n",
    "    stop = start + words_period+1\n",
    "    starting = ending\n",
    "    ending = starting + time_period\n",
    "    \n",
    "if os.path.exists(os.path.join(current_path, \"mod-et-emo-005.csv\")):\n",
    "    os.remove(os.path.join(current_path, \"mod-et-emo-005.csv\"))\n",
    "df = pd.DataFrame(data = {'Sentences': strings, 'Time Period':period_num, 'Time from, time to (in miliseconds)':from_to, 'Detected polarity': polarity, 'Emotional_Intensity': emotional_intensity})\n",
    "df.to_csv('mod-et-emo-005.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mod-et-emo-006 ==> Polarity aggregation in subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranks: [4, 3, 1, 2, 5]\n",
      "Final Result: [0.11956521739130435, 0.18478260869565216, 0.3804347826086957, 0.2717391304347826, 0.043478260869565216]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ranking is being performed based upon how many times an emotion appears. Most frequent will get rank 1 and least\n",
    "frequent will get rank 5.\n",
    "\"\"\"\n",
    "\n",
    "ranks = [0,0,0,0,0]\n",
    "rep = count_repitition.copy()\n",
    "total = sum(rep)\n",
    "scores = []\n",
    "for i in range (len(rep)):\n",
    "    scores.append(rep[i]/total)\n",
    "rank = 1\n",
    "for i in range (len(count_repitition)):\n",
    "    value = max(count_repitition)\n",
    "    ranks [rep.index(value)] = ranks [rep.index(value)] + rank\n",
    "    count_repitition.remove(value)\n",
    "    rank = rank + 1\n",
    "    \n",
    "print(\"Ranks: \"+str(ranks))\n",
    "print(\"Final Result: \"+str(scores))\n",
    "\n",
    "if os.path.exists(os.path.join(current_path, \"mod-et-emo-006.csv\")):\n",
    "    os.remove(os.path.join(current_path, \"mod-et-emo-006.csv\"))\n",
    "df = pd.DataFrame(data = {'Emotions': emotions, 'Ranking':ranks, 'Final Result':scores})\n",
    "df.to_csv('mod-et-emo-006.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mood3",
   "language": "python",
   "name": "mood3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
